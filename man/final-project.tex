\documentclass{article} % For LaTeX2e
\usepackage{nips12submit_e,times}
%\documentstyle[nips12submit_09,times,art10]{article} % For LaTeX 2.09

\usepackage{amsmath, amsthm, amssymb}
% \usepackage{natbib}
\usepackage{graphicx}
\usepackage{wrapfig}
\usepackage{subcaption}

\pagestyle{empty}

\title{Optimal strategies in mental rotation tasks}
\author{Jessica B.~Hamrick\\
  Department of Psychology\\
  University of California, Berkeley\\
  Berkeley, CA 94720\\
  \texttt{jhamrick@berkeley.edu}}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\TODO}[1]{\textcolor{red}{[TODO: #1]}}

\nipsfinalcopy % Uncomment for camera-ready version

\begin{document}

\maketitle

\begin{abstract}
\TODO{}
\end{abstract}

\section{Introduction}

% 1. The big picture
% 2. Our contribution
% 3. How we do it
% 4. What's in the paper

Consider the objects in Figure \ref{fig:mental-rotation}. In each
panel, are the two depicted objects identical (except for a rotation),
or distinct? When presented with this mental rotation task, people
default to a strategy in which they visualize one object rotating
until it is congruent with the other \cite{Shepard1971}. There is
strong evidence for such \textit{mental imagery} or \textit{mental
  simulation}: we can imagine three-dimensional objects in our minds
and manipulate them, to a certain extent, as if they were real
\cite{Kosslyn:2009tj}.

However, the use of mental simulation is predicated on determining
appropriate parameters to give the simulation, and people's cognitive
constraints may furthermore place limitations on the duration or
precision of simulation. One hypothesis for how these issues are
handled argues that people use a ``rational'' solution, meaning that
it is optimal under the given constraints
\cite{Lieder:2012wg,Vul:2009wy,Griffiths2012a}.

In the case of the classic mental rotation task, we might ask: in what
direction should the object be rotated?  What are the requirements for
``congruency''? When should one stop rotating and accept the
hypothesis that the objects are different? In this project, I will
investigate the optimal computational solution to this task. Thus, the
specific question this project aims to answer is: what is the rational
computational solution to the mental rotation task, and does it
predict the people's behavior on the task?

\section{Background}

% We do not know the form of $S$, and so cannot compute this integral
% analytically. We cannot rely on a simple Monte Carlo simulation to
% estimate it, either: because mental rotation is costly in terms of
% cognitive resources and must be performed in a sequence, we cannot
% evaluate $S(I_b|I_M)$ at arbitrary $I_M$. Instead, we must choose a
% relatively small, sequential set of $I_M$ to estimate the
% integral. 

% Bayesian quadrature \cite{Diaconis:1988uo} provides an alternate
% method for evaluating the integral by placing a prior distribution on
% functions and then computing a posterior over values of the
% integral. While Bayesian quadrature itself does not address the issue
% of choosing points to evaluate the function at, recent work in
% statistics and machine learning has examined how to optimally select
% these samples \cite{Osborne:2012tm}.

\begin{figure}[t]
  \centering
  \includegraphics[width=0.95\textwidth]{../figures/shepard-rotation.png}
  \caption{\textbf{Classic mental rotation task}. Participants in
    \cite{Shepard1971} saw stimuli such as these, and judged whether
    each pair of shapes was the same shape in two different
    orientations, or two different shapes. \textbf{A} shows a ``plane
    rotation'', \textbf{B} shows a ``depth rotation'', and \textbf{C}
    shows two distinct objects. \TODO{include Shepard results, too?}}
  \label{fig:mental-rotation}
\end{figure}

\section{Computational model}

People are presented with two images, $X_a$ and $X_b$, which are the
coordinates of the vertices of two shapes (e.g., see Figure
\ref{fig:stimuli}). Participants must determine whether $X_a$ and
$X_b$ were generated from the same (albeit possibly transformed and
permuted) original shape, i.e., whether $\exists R,M\textrm{ s.t. }
X_b=MRX_a$, where $M$ is a permutation matrix and $R$ is a rotation
matrix.

We can formulate the judgment of whether $X_a$ and $X_b$ have the same
origins by deciding about two hypotheses:

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  $h_0$: $\forall M,R\ X_b\neq MRX_a$
\item
  $h_1$: $\exists M,R\textrm{ s.t. } X_b=MRX_a$
\end{itemize}

To compare the hypotheses, we need to compute the likelihood of each
hypothesis, i.e. $p(X_a,\ X_b\ \vert \ h_0)$ and $p(X_a,\ X_b\ \vert \
h_1)$.

The likelihood is easy to compute under $h_0$, because we assume that
$X_a$ and $X_b$ are independent. Thus:
\begin{equation}
  p(X_a,\ X_b\ \vert \ h_0)=p(X_a)p(X_b)
  \label{eq:lh-h0}
\end{equation}

When the configurations are the same, the likelihood becomes more
complicated:
\begin{equation} 
  p(X_a,\ X_b\ \vert \ h_1)=\int_R\int_M p(X_a) p(X_b\vert X_a,R,M) p(R) p(M)\ \mathrm{d}M\ \mathrm{d}R
\end{equation}

For a small number of vertices, we can compute the integral over $M$
by enumerating every possible mapping between $X_a$ and $X_b$. After
doing so, we obtain:
\begin{equation} 
  p(X_a,\ X_b\ \vert \ h_1)=\int_R p(X_a) p(X_b\vert X_a,R) p(R)\ \mathrm{d}R
\end{equation}

However, we cannot compute $p(X_b\vert X_a, R)$ directly. Instead, we
introduce a new variable $X_R$ denoting a mental image, which
approximates $RX_a$. The $X_R$ are generated sequentially by repeated
application of a function $\tau$:
\begin{align}
  X_R&=RX_a\nonumber \\
  &=\tau(X_{R-r}, r)\nonumber \\
  &=\tau(\tau(X_{R-2r}, r), r)\nonumber \\
  &\ldots{}\nonumber \\
  &=\tau^{(\frac{R}{r})}(X_a, r)
  \label{eq:tau}
\end{align} 
Where $r$ is a small angle, and $\tau^{(i)}$ indicates $i$ recursive
applications of $\tau$. Using this sequential function, we get:
\begin{align}
  p(X_a, X_b\ \vert \ h_1)&=\int_R \int_{X} p(X_b\vert X) p(X\vert X_a, R)p(X_a)p(R)\ \mathrm{d}X\ \mathrm{d}R \nonumber \\
  x&= \int_R \int_X p(X_b\vert X)\delta(\tau^{(\frac{R}{r})}(X_a, r)-X)p(X_a)p(R)\ \mathrm{d}X\ \mathrm{d}R \nonumber \\
  &= \int_R p(X_b\vert X_R)p(X_a)p(R)\ \mathrm{d}R
  \label{eq:lh-h1}
\end{align}

Once we have computed both likelihoods, we compute their ratio:
\begin{equation}
  \ell=\frac{p(X_a, X_b\ \vert \ h_1)}{p(X_a, X_b\ \vert \ h_0)}=\frac{\int_R p(X_b\ \vert\ X_R)p(R)\ \mathrm{d}R}{p(X_b)}
  \label{eq:lh-ratio}
\end{equation}
If $\ell<1$, then $h_0$ is the more likely hypothesis. If $\ell>1$,
then $h_1$ is the more likely hypothesis.

\section{Implementation}


\begin{figure}[t]
  \centering
  \begin{subfigure}[b]{0.45\textwidth}
    \centering
    \includegraphics[width=\textwidth]{../figures/stimuli_shapes.pdf}
    \vspace{0pt}
    \caption{Example stimuli}
    \label{fig:stimuli}
  \end{subfigure}
  \begin{subfigure}[b]{0.45\textwidth}
    \centering
    \includegraphics[width=\textwidth]{../figures/likelihood_function.pdf}
    \caption{Likelihood function}
    \label{fig:likelihood}
  \end{subfigure}
  \caption{\textbf{Rotated shapes and their similarity.}  \textbf{(a)}
    An example stimulus in which the shapes differ only by a
    rotation. All stimuli consist of five vertices centered around the
    origin, and four edges which create a closed loop from the
    vertices. \textbf{(b)} The approximate likelihood (similarity) of
    $X_b$ given $X_R$, where $X_R$ is a rotation of $X_a$ by the angle
    $R$. The true angle of rotation between $X_a$ and $X_b$ is
    approximately $\frac{\pi}{4}$, corresponding to the global maximum
    in the likelihood function.}
\end{figure}

We define the prior probabilities over stimuli based on how they are
generated. For shapes with $n$ vertices, each vertex is at a random
angle with a radius random chosen between 0 and 1, in polar
coordinates, and could be chosen in any of $n!$ different ways. Thus,
the prior over a shape $X$ is:
\begin{equation}
  p(X)=n!\left(\frac{1}{2\pi}\right)^n
  \label{eq:prior}
\end{equation} 
which gives us the denominator in Equation \ref{eq:lh-ratio}. 

Computing the numerator of Equation \ref{eq:lh-ratio} is more
difficult, as we do know $p(X_b\vert X_R)$. We approximate it with a
similarity function $S(X_b, X_R)$, which also takes into account the
different possible mappings of vertices:
\begin{equation}
  Z=\int_R S(X_b, X_R)p(R)\ \mathrm{d}R\ \approx\int_Rp(X_b\ \vert\ X_R)p(R)\ \mathrm{d}R
  \label{eq:Z}
\end{equation}

Because the vertices are connected in a way which forms a closed loop,
we need only consider $n$ mappings, $M$, of the $n$ vertices (we
assume the uncertainty is only in which is the ``first''
vertex). Thus, the possible orderings are of the form $M=\lbrace{}0,
1, \ldots{}, n\rbrace{}$, $M=\lbrace{}n, 0, \ldots{}, n-1\rbrace{}$,
and so on. This gives us an explicit form for the similarity function:
\begin{equation}
  S(X_b, X_R)=\frac{1}{n}\sum_{M\in\mathbb{M}}\prod_{i=1}^n\mathcal{N}(X_b[i]\ \vert \ (MX_R)[i], \Sigma)
  \label{eq:similarity}
\end{equation}
where $i$ denotes the $i^{th}$ vertex. An example of $S$ for the
stimuli shown in Figure \ref{fig:stimuli} is illustrated in Figure
\ref{fig:likelihood}.

This process of mental rotation (i.e., generating a single $X_R$ (as
in Equation \ref{eq:tau}) and then computing $S(X_b, X_R)$) is a
computationally demanding cognitive process. Thus, our goal is to
minimize the number of rotations while still obtaining an estimate of
$Z$ that is accurate enough to choose the correct hypothesis. With
this in mind, we now examine several approaches to estimating $Z$.

In each of these models, we denote the computed rotations to be a set
$\mathbf{R}=\{R_1, R_2, \ldots{}\}$.

\subsection{Gold standard}

To compare the accuracy of other models' estimates of $Z$, we computed
a ``gold standard''\footnote{This only gives an accurate estimate of
  $Z$, which is itself an approximation, and is thus not necessarily
  the true value of the numerator in Equation \ref{eq:lh-ratio}.} by
evaluating $S(X_b, X_R)$ at 360 values of $R$ spaced evenly between
$0$ and $2\pi$.

\subsection{Na\"ive model}

\begin{figure}[t]
  \centering
  \begin{subfigure}[b]{0.45\textwidth}
    \centering
    \includegraphics[width=\textwidth]{../figures/li_regression.pdf}
    \caption{Na\"ive model}
    \label{fig:li}
  \end{subfigure}
  \begin{subfigure}[b]{0.45\textwidth}
    \centering
    \includegraphics[width=\textwidth]{../figures/vm_regression.pdf}
    \caption{Parametric model}
    \label{fig:vm}
  \end{subfigure}
  \caption{\TODO{}}
\end{figure}

As a baseline, we defined a na\"ive model which performs a
hill-climbing search over the similarity function until it reaches a
(possibly local) maximum. Once a maximum as been found, the model
computes an estimate of $Z$ by linearly interpolating between sampled
rotations. Figure \ref{fig:li} shows an example of the na\"ive model
prior to estimating $Z$.

\subsection{Parametric (Von Mises) model}

Another strategy is to assume a parametric shape for $S$ and fit the
appropriate parameters. When $h_1$ is correct, it is likely that the
function will be approximately unimodal (shapes with rotational
symmetry would be multimodal). A reasonable assumption, then, is that
the likelihood follows a ``wrapped Gaussian'', or Von Mises,
distribution:
\begin{equation}
  S(X_b, X_R) \approx h\cdot{}p(R\ \vert\ \hat{\theta}, \kappa)=\frac{h}{2\pi X_0(\kappa)}e^{\kappa\cos(R-\hat{\theta})}
\end{equation}
where $\kappa$ is the concentration parameter, $\hat{\theta}$ is the
preferred direction, and $h$ is a scale parameter. We fit these
parameters by minimizing the mean squared error between this PDF and
the computed values of $S$. To choose the sequence of rotations, we
use the same hill-climbing strategy as in the na\"ive model.\TODO{a
  better strategy would have been to compute the variance in parameter
  estimates, or something...} Figure \ref{fig:vm} shows an example of
this parametric model, prior to estimating $Z$.

\subsection{Nonparametric (Bayesian Quadrature) model}

\begin{figure}[t]
  \centering
  \includegraphics[width=\textwidth]{../figures/bq_regression.pdf}
  \caption{\textbf{Nonparametric model.} \TODO{}}
  \label{fig:bq}
\end{figure}

A more flexible strategy uses what is known as \emph{Bayesian
  quadrature} \cite{Diaconis:1988uo,OHagan:1991tx} to estimate $Z$.
Bayesian quadrature allows us to compute a posterior distrbution over
$Z$ by placing a Gaussian Process (GP) \cite{Rasmussen:2006vz} prior
on the function $S$ and evaluating $S$ at a particular set of
points. However, this strategy has its difficulties: while in our
case, $S$ is a non-negative likelihood function, GP regression
enforces no such constraint.

\cite{Osborne:2012tm} give a method which more often (but not
entirely) avoids this problem. Instead of placing a prior over $S$, a
prior is instead placed over $\log S$, thus ensuring that $S=e^{\log
  S}$ will be positive\footnote{In practice, as in
  \cite{Osborne:2012tm}, we use the transform of $\log(S+1)$.}:
\begin{equation*}
  E[Z\ \vert \ \log S]=\int_{\log S}\left(\int_R \exp(\log{S(X_b,X_R)})p(R)\ \mathrm{d}R\right)\mathcal{N}\left(\log{S}\ \vert \ \mu_{\log S}, \Sigma_{\log S}\right)\ \mathrm{d}\log S
\end{equation*}
where $\mu_{\log S}$ and $\Sigma_{\log S}$ are the mean and
covariance, respectively, of the GP regression over $\log S$ given
$\mathbf{R}$. We approximate this according to the method given in
\cite{Osborne:2012tm}:
\begin{equation}
  \mu_Z=E[Z\ \vert \ S, \log S, \Delta_c] \approx \int_R \mu_{S}(1 + \mu_\Delta) p(R)\ \mathrm{d}R 
  \label{eq:bq-Z-mean}
\end{equation}
where $\mu_S$ is the mean of a GP regression over $S$ given
$\mathbf{R}$; and $\mu_{\Delta_c}$ is a regression over
$\Delta_c=\mu_{\log S} - \log \mu_S$ given $\mathbf{R}_c$, which
consists of $\mathbf{R}$ and a set of intermediate \emph{candidate
  points}\footnote{These candidate points do not require evaluating
  the true $S$, only the GP estimates of $S$ and $\log S$.} $c$ as
described in \cite{Osborne:2012tm}. The variance of the estimate of
$Z$ is given by:
\begin{equation}
  \sigma_Z=\mathrm{Var}(Z\ \vert \ S, \log S, \Delta_c) = \int_R\int_{R^\prime} \mu_S(R)\mu_S(R^\prime) \Sigma_{\log S}(R, R^\prime)p(R)p(R^\prime)\ \mathrm{d}R\ \mathrm{d}R^\prime
  \label{eq:bq-Z-var}
\end{equation}

To choose $\mathbf{R}$, we pick an initial direction of rotation which
results in the higher value of $S$, and then continue rotating in that
direction until the variance in Equation \ref{eq:bq-Z-var} is low
enough that we are confident about the likelihood ratio $\ell$:
\begin{equation*}
p(\ell)\approx\frac{1}{p(X_b)}\ \mathcal{N}(Z\ \vert\ \mu_Z, \sigma_z)
\end{equation*}
Specifically, we choose $h_0$ when $p(\ell < 1)\geq 0.95$, and we
choose $h_1$ when $p(\ell > 1)\geq 0.95$. Until one of these
conditions are met (or the shape has been fully rotated), the model
will continue to compute rotations and update its estimate of $Z$.

\section{Results}

We evaluated each model's performance on 50 randomly generated shapes
(such as $X_a$ in Figure \ref{fig:stimuli}). For each shape, we picked
a random angle $R$ and generated two stimuli: a $h_0$ stimulus, by
reflecting $X_a$ across the $y$-axis and rotating it by $R$; and a
$h_1$ stimulus, by simply rotating $X_a$ by $R$.

We considered three metrics of performance in particular:
\begin{itemize}
\item \textit{Rotations}: for stimuli where $h_1$ is correct, how
  correlated are the true angles of rotation with the rotations
  performed by the model? This is defined to be the Pearson's
  correlation coefficient $\rho$ between $R$ and $\vert
  \mathbf{R}\vert$. Figure \ref{fig:rotations} shows individual points
  corresponding to true rotation vs. rotation by the model, for each
  model.
\item \textit{Responses}: how accurate is the model at choosing the
  correct hypothesis? This is defined to be the mean error
  ($\mathrm{ME}$), or fraction of times the model picks the incorrect
  hypothesis.
\item \textit{Estimates of $Z$}: how accurate is the model's estimate
  of $Z$? This is defined to be the mean squared error
  ($\mathrm{MSE}$) between the model's estimate of $Z$ and the ``gold
  standard'' value, where the error has scale such that
  $\mathrm{MSE}=0$ indicates no error, and $\mathrm{MSE}=1$ indicates
  maximum error. Figure \ref{fig:accuracy} shows plots of the true
  (gold standard) value of $Z$ vs. the model's estimate, for each
  model.
\end{itemize}

\paragraph{Gold standard} 

The gold standard always evaluates $S$ at all angles, so the
correlation between it and the true angle of rotation is
undefined. The mean error of responses was $\mathrm{ME}=0.01$,
indicating that $Z$ is a sufficient approximation\footnote{The one
  error comes from a $h_0$ stimulus which is highly
  symmetric. Although it is distinct enough for the author to
  correctly judge when both images are viewed side by side, it remains
  to be seen whether human participants, viewing the images
  sequentially would also make the correct judgment.}  of $\int_R
p(X_b\ \vert\ X_R)p(R)\ \mathrm{d}R$.

\paragraph{Na\"ive model} 

The correlation between the number of rotations computed by the
na\"ive model and the true angle of rotation was $\rho=-0.11$. The
shape of the data is more interesting (see Figure \ref{fig:rotations},
right panel): the na\"ive model actually corresponds quite well to the
true angle of rotation for $R<\frac{\pi}{2}$ ($\rho=0.63$). This is
unsurprising, because the closer the true angle is to zero, the less
the model has to rotate, and the less likely it will get stuck on
local maxima. Thus, it is more likely to locat the global maximum,
which generally corresponds to the true angle of rotation. For
$R>\frac{\pi}{2}$, we see an increasing tendency to under-rotate
($\rho=-0.42$); this is likely because it finds a local maxima and
ends prematurely.

The na\"ive model's response error rate was better than chance, but
still quite high, with $\mathrm{ME}=0.23$. Closer inspection reveals
that the bulk of this comes from $h_1$ stimuli ($\mathrm{ME}=0.36$
vs. $\mathrm{ME}=0.1$ for $h_0$ stimuli). This is, as above, probably
related to finding only local maxima: if the model finds a local
maxima which is low enough, the area under the estimated curve will
not be large enough to accept $h_1$.

This is reflected in the accuracy in estimating $Z$ as well
($\mathrm{MSE}=0.14$). Figure \ref{fig:accuracy} (leftmost panel)
shows individual estimates for each stimulus, color-coded by true
hypothesis. The model underestimates $Z$ for significant portion of
the $h_1$ stimuli. On the other $h_1$ stimuli, the na\"ive model
actually tends to overestimate because linear interpolation does not
account for symmetry of global maxima (\TODO{figure?}).

\paragraph{Parametric (Von Mises) model}

Like the na\"ive model, the parametric model uses hill-climbing as a
sampling strategy, so they have identical correlations with the true
angle of rotation. However, given the same $\mathbf{R}$, the
parametric model estimates $Z$ differently, thus giving rise to
different response and estimated $Z$ error rates. The response error
for the parametric model was higher than the na\"ive model, with
$\mathrm{ME}=0.26$. Breaking this down, we see that the error for
$h_1$ stimuli is $\mathrm{ME}=0.5$: in other words, the parametric
model is at chance when determining whether two identical shapes are
the same. This is because it never overestimates (see Figure
\ref{fig:accuracy}, center panel). So, it accurately estimates $Z$ for
stimuli which the na\"ive model would overestimate
($\mathrm{MSE}=0.07$), but still has poor performance when $Z$ is
underestimated. Indeed, if we exclude stimuli for which the nai\"ive
model overestimates ($Z\geq 3$), the error for the na\"ive model
lowers to match that of the parametric model.

\paragraph{Nonparametric (Bayesian Quadrature) model}



\begin{figure}[t]
  \centering
  \includegraphics[width=\textwidth]{../figures/model_rotations.pdf}
  \caption{\textbf{Model rotations.} \TODO{}}
  \label{fig:rotations}
\end{figure}

\begin{figure}[t]
  \centering
  \includegraphics[width=\textwidth]{../figures/Z_accuracy.pdf}
  \caption{\textbf{Accuracy in estimating $Z$.} \TODO{}}
  \label{fig:accuracy}
\end{figure}

\section{Discussion}


% \renewcommand\bibsection{\subsubsection*{\refname}}
\renewcommand\refname{\normalsize{References}}
\bibliographystyle{ieeetr}
\bibliography{references}

\end{document}



